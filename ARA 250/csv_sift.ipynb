{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import string\n",
    "import nltk\n",
    "import json\n",
    "import re\n",
    "from datetime import datetime\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from pyarabic import araby\n",
    "from pyarabic.araby import strip_tashkeel\n",
    "from collections import Counter, defaultdict\n",
    "import xlsxwriter\n",
    "import arabic_reshaper\n",
    "from bidi.algorithm import get_display\n",
    "import openpyxl\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial code that outputs to a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Sudan_almeghar_19.csv\n",
      "Processing file: Sudan_almeghar_20.csv\n",
      "Processing file: Sudan_almeghar_21.csv\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_search_arabic_text(directory_path, output_dir, country_name, newspaper_name, search_terms, max_articles_per_file=500):\n",
    "    file_count = 1\n",
    "    article_count = 0\n",
    "    output_file = None\n",
    "    output_file_path = lambda count: os.path.join(output_dir, f\"{country_name}_{newspaper_name}_search_results_part{count}.txt\")\n",
    "    search_terms_set = set(search_terms)  # Prepare search terms\n",
    "    \n",
    "    def is_relevant_file(filename):\n",
    "        return country_name in filename and newspaper_name in filename and filename.endswith('.csv')\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if is_relevant_file(file_name):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "                csv_reader = csv.DictReader(file, delimiter='\\t')\n",
    "                print(\"Processing file:\", file_name)\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    if output_file is None or article_count >= max_articles_per_file:\n",
    "                        if output_file is not None:\n",
    "                            output_file.close()\n",
    "                        output_file = open(output_file_path(file_count), 'w', encoding='utf-8')\n",
    "                        file_count += 1\n",
    "                        article_count = 0\n",
    "\n",
    "                    text = row.get('Text', '')\n",
    "                    normalized_text = araby.strip_tashkeel(text)  # Normalize Arabic text\n",
    "                    tokens = word_tokenize(normalized_text)\n",
    "                    \n",
    "                    found_terms = search_terms_set.intersection(tokens)\n",
    "                    \n",
    "                    if found_terms:\n",
    "                        title = row.get('Title', 'No Title Found')\n",
    "                        output_file.write(f\"Title: {title}\\nFound Terms: {', '.join(found_terms)}\\nText: {text}\\n\\n\")\n",
    "                        article_count += 1\n",
    "\n",
    "    if output_file is not None:\n",
    "        output_file.close()\n",
    "\n",
    "# Example usage\n",
    "directory_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Sudan\"  # Directory containing CSV files\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Sudan\"  # Output directory\n",
    "country_name = 'Sudan'\n",
    "newspaper_name = 'almeghar'\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']  # Arabic terms to search for\n",
    "tokenize_and_search_arabic_text(directory_path, output_dir, country_name, newspaper_name, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Sudan_almeghar_19.csv\n",
      "Processing file: Sudan_almeghar_20.csv\n",
      "Processing file: Sudan_almeghar_21.csv\n",
      "Total articles: 4053\n",
      "COVID-19 related articles: 5\n",
      "Proportion of COVID-19 related articles: 0.00\n"
     ]
    }
   ],
   "source": [
    "def tokenize_search_and_context(directory_path, output_dir, country_name, newspaper_name, search_terms, max_articles_per_file=500, window=10):\n",
    "    file_count = 1\n",
    "    article_count = 0\n",
    "    total_article_count = 0\n",
    "    covid_article_count = 0\n",
    "    output_file = None\n",
    "    output_file_path = lambda count: os.path.join(output_dir, f\"{country_name}_{newspaper_name}_search_results_part{count}.txt\")\n",
    "    search_terms_set = set(search_terms)  # Prepare search terms\n",
    "    \n",
    "    def is_relevant_file(filename):\n",
    "        return country_name in filename and newspaper_name in filename and filename.endswith('.csv')\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if is_relevant_file(file_name):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "                csv_reader = csv.DictReader(file, delimiter='\\t')\n",
    "                print(\"Processing file:\", file_name)\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    total_article_count += 1\n",
    "                    if output_file is None or article_count >= max_articles_per_file:\n",
    "                        if output_file is not None:\n",
    "                            output_file.close()\n",
    "                        output_file = open(output_file_path(file_count), 'w', encoding='utf-8')\n",
    "                        file_count += 1\n",
    "                        article_count = 0\n",
    "\n",
    "                    text = row.get('Text', '')\n",
    "                    normalized_text = text.lower()  # Normalize Arabic text by lowercasing\n",
    "                    tokens = word_tokenize(normalized_text)\n",
    "                    \n",
    "                    found_terms = search_terms_set.intersection(tokens)\n",
    "                    if found_terms:\n",
    "                        covid_article_count += 1\n",
    "                        title = row.get('Title', 'No Title Found')\n",
    "                        # Find context around each found term\n",
    "                        for term in found_terms:\n",
    "                            indices = [i for i, token in enumerate(tokens) if token == term]\n",
    "                            for index in indices:\n",
    "                                start = max(0, index - window)\n",
    "                                end = min(len(tokens), index + window + 1)\n",
    "                                context = ' '.join(tokens[start:end])\n",
    "                                output_file.write(f\"Title: {title}\\nTerm: {term}\\nContext: {context}\\n\\n\")\n",
    "                        article_count += 1\n",
    "\n",
    "    if output_file is not None:\n",
    "        output_file.close()\n",
    "\n",
    "    # Calculate and print the proportion of COVID-19 related articles\n",
    "    if total_article_count > 0:\n",
    "        proportion_covid = covid_article_count / total_article_count\n",
    "        print(f\"Total articles: {total_article_count}\")\n",
    "        print(f\"COVID-19 related articles: {covid_article_count}\")\n",
    "        print(f\"Proportion of COVID-19 related articles: {proportion_covid:.2f}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Sudan\"  # Directory containing CSV files\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Sudan\"  # Output directory\n",
    "country_name = 'Sudan'\n",
    "newspaper_name = 'almeghar'\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']  # Arabic terms to search for\n",
    "tokenize_search_and_context(directory_path, output_dir, country_name, newspaper_name, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to excel, country newspaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_search_and_context_to_excel(directory_path, output_dir, country_name, newspaper_name, search_terms, max_articles_per_file=500, window=10):\n",
    "    results = []\n",
    "    search_terms_set = set(search_terms)  # Prepare search terms\n",
    "    \n",
    "    def is_relevant_file(filename):\n",
    "        return country_name in filename and newspaper_name in filename and filename.endswith('.csv')\n",
    "\n",
    "    for file_name in os.listdir(directory_path):\n",
    "        if is_relevant_file(file_name):\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "                csv_reader = csv.DictReader(file, delimiter='\\t')\n",
    "                print(\"Processing file:\", file_name)\n",
    "\n",
    "                for row in csv_reader:\n",
    "                    text = row.get('Text', '')\n",
    "                    normalized_text = text.lower()  # Normalize Arabic text by lowercasing\n",
    "                    tokens = word_tokenize(normalized_text)\n",
    "                    \n",
    "                    found_terms = search_terms_set.intersection(tokens)\n",
    "                    if found_terms:\n",
    "                        title = row.get('Title', 'No Title Found')\n",
    "                        # Find context around each found term\n",
    "                        for term in found_terms:\n",
    "                            indices = [i for i, token in enumerate(tokens) if token == term]\n",
    "                            for index in indices:\n",
    "                                start = max(0, index - window)\n",
    "                                end = min(len(tokens), index + window + 1)\n",
    "                                context = ' '.join(tokens[start:end])\n",
    "                                results.append({\n",
    "                                    \"Title\": title,\n",
    "                                    \"Term\": term,\n",
    "                                    \"Context\": context,\n",
    "                                    \"Filename\": file_name\n",
    "                                })\n",
    "\n",
    "    # Create a DataFrame and write to Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file_path = os.path.join(output_dir, f\"{country_name}_{newspaper_name}_search_results.xlsx\")\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    print(f\"Results written to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "directory_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Sudan\"\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"\n",
    "country_name = 'Sudan'\n",
    "newspaper_name = 'almeghar'\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']\n",
    "tokenize_search_and_context_to_excel(directory_path, output_dir, country_name, newspaper_name, search_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to excel, whole country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Sudan_almashhadalsudani_19.csv\n",
      "Processing file: Sudan_almashhadalsudani_20.csv\n",
      "Processing file: Sudan_almashhadalsudani_21.csv\n",
      "Processing file: Sudan_almeghar_19.csv\n",
      "Processing file: Sudan_almeghar_20.csv\n",
      "Processing file: Sudan_almeghar_21.csv\n",
      "Processing file: Sudan_alnilin_19.csv\n",
      "Processing file: Sudan_alnilin_20.csv\n",
      "Processing file: Sudan_alnilin_21.csv\n",
      "Processing file: Sudan_alrakoba_19.csv\n",
      "Processing file: Sudan_alrakoba_20.csv\n",
      "Processing file: Sudan_alrakoba_21.csv\n",
      "Processing file: Sudan_alsadda_19.csv\n",
      "Processing file: Sudan_alsadda_20.csv\n",
      "Processing file: Sudan_alsudani_19.csv\n",
      "Processing file: Sudan_alsudani_21.csv\n",
      "Processing file: Sudan_assayha_21.csv\n",
      "Processing file: Sudan_sudantribune_19.csv\n",
      "Processing file: Sudan_sudantribune_20.csv\n",
      "Processing file: Sudan_sudantribune_21.csv\n",
      "Processing file: Sudan_suna_19.csv\n",
      "Processing file: Sudan_suna_20.csv\n",
      "Processing file: Sudan_suna_21.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\search_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "def tokenize_search_and_context_to_excel(country_path, output_dir, search_terms, max_articles_per_file=500, window=10):\n",
    "    results = []\n",
    "    search_terms_set = set(search_terms)  # Prepare search terms\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(country_path):\n",
    "        for file_name in filenames:\n",
    "            if file_name.endswith('.csv'):\n",
    "                file_path = os.path.join(dirpath, file_name)\n",
    "                with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "                    csv_reader = csv.DictReader(file, delimiter='\\t')\n",
    "                    print(\"Processing file:\", file_name)\n",
    "\n",
    "                    for row in csv_reader:\n",
    "                        text = row.get('Text', '')\n",
    "                        normalized_text = text.lower()  # Normalize Arabic text by lowercasing\n",
    "                        tokens = word_tokenize(normalized_text)\n",
    "                        \n",
    "                        found_terms = search_terms_set.intersection(tokens)\n",
    "                        if found_terms:\n",
    "                            title = row.get('Title', 'No Title Found')\n",
    "                            # Find context around each found term\n",
    "                            for term in found_terms:\n",
    "                                indices = [i for i, token in enumerate(tokens) if token == term]\n",
    "                                for index in indices:\n",
    "                                    start = max(0, index - window)\n",
    "                                    end = min(len(tokens), index + window + 1)\n",
    "                                    context = ' '.join(tokens[start:end])\n",
    "                                    results.append({\n",
    "                                        \"Title\": title,\n",
    "                                        \"Term\": term,\n",
    "                                        \"Context\": context,\n",
    "                                        \"Filename\": file_name,\n",
    "                                        \"Newspaper\": os.path.basename(dirpath)  # Newspaper name from directory\n",
    "                                    })\n",
    "\n",
    "    # Create a DataFrame and write to Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file_path = os.path.join(output_dir, \"search_results.xlsx\")\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    print(f\"Results written to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "country_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Sudan\"  # Country directory\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"  # Output directory\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']  # Arabic terms to search for\n",
    "tokenize_search_and_context_to_excel(country_path, output_dir, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include all headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_search_and_context_to_excel(country_path, output_dir, search_terms, max_articles_per_file=500, window=10):\n",
    "    results = []\n",
    "    search_terms_set = set(search_terms)  # Prepare search terms\n",
    "\n",
    "    def is_relevant_file(filename):\n",
    "        return filename.endswith('.csv')\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(country_path):\n",
    "        for file_name in filenames:\n",
    "            if is_relevant_file(file_name):\n",
    "                file_path = os.path.join(dirpath, file_name)\n",
    "                with open(file_path, mode='r', encoding='utf-8') as file:\n",
    "                    csv_reader = csv.DictReader(file, delimiter='\\t')\n",
    "                    for row in csv_reader:\n",
    "                        text = row.get('Text', '').lower()\n",
    "                        tokens = word_tokenize(text)\n",
    "                        \n",
    "                        found_terms = search_terms_set.intersection(tokens)\n",
    "                        if found_terms:\n",
    "                            # Collect all metadata from the row\n",
    "                            context_data = {\n",
    "                                \"Text\": row.get('Text', 'No Text Found'),\n",
    "                                \"Title\": row.get('Title', 'No Title Found'),\n",
    "                                \"URL\": row.get('URL', 'No URL Provided'),\n",
    "                                \"Date\": row.get('Date', 'No Date Provided'),\n",
    "                                \"Category\": row.get('Category', 'No Category Provided'),\n",
    "                                \"Newspaper\": os.path.basename(dirpath),\n",
    "                                \"File_Name\": file_name\n",
    "                            }\n",
    "                            # Find context around each found term\n",
    "                            for term in found_terms:\n",
    "                                indices = [i for i, token in enumerate(tokens) if token == term]\n",
    "                                for index in indices:\n",
    "                                    start = max(0, index - window)\n",
    "                                    end = min(len(tokens), index + window + 1)\n",
    "                                    context = ' '.join(tokens[start:end])\n",
    "                                    result = context_data.copy()\n",
    "                                    result.update({\n",
    "                                        \"Term\": term,\n",
    "                                        \"Context\": context\n",
    "                                    })\n",
    "                                    results.append(result)\n",
    "\n",
    "    # Create a DataFrame and write to Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    output_file_path = os.path.join(output_dir, \"Iraq_search_results.xlsx\")\n",
    "    df.to_excel(output_file_path, index=False)\n",
    "    print(f\"Results written to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "country_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Iraq\"  # Directory containing CSV files\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"  # Output directory\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']  # Arabic terms to search for\n",
    "tokenize_search_and_context_to_excel(country_path, output_dir, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: Iraq_albayyna_new_19.csv\n",
      "Processing file: Iraq_albayyna_new_20.csv\n",
      "Processing file: Iraq_albayyna_new_21.csv\n",
      "Processing file: Iraq_alliraqnews_19.csv\n",
      "Processing file: Iraq_alliraqnews_20.csv\n",
      "Processing file: Iraq_alliraqnews_21.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Iraq_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "def tokenize_search_and_context_to_csv(country_path, output_dir, search_terms, window=10):\n",
    "    search_terms_set = set(search_terms)  # Prepare search terms\n",
    "    results_header = [\"Text\", \"Title\", \"URL\", \"Date\", \"Category\", \"Newspaper\", \"File_Name\", \"Term\", \"Context\"]\n",
    "\n",
    "    # Prepare the output CSV file\n",
    "    output_file_path = os.path.join(output_dir, \"Iraq_search_results.csv\")\n",
    "    with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=results_header)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Process each file in the directory\n",
    "        def is_relevant_file(filename):\n",
    "            print(\"Processing file:\", filename)\n",
    "            return filename.endswith('.csv')\n",
    "\n",
    "        for dirpath, dirnames, filenames in os.walk(country_path):\n",
    "            for file_name in filenames:\n",
    "                if is_relevant_file(file_name):\n",
    "                    file_path = os.path.join(dirpath, file_name)\n",
    "                    with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "                        csv_reader = csv.DictReader(infile, delimiter='\\t')\n",
    "                        for row in csv_reader:\n",
    "                            text = row.get('Text', '').lower()\n",
    "                            tokens = word_tokenize(text)\n",
    "                            found_terms = search_terms_set.intersection(tokens)\n",
    "                            if found_terms:\n",
    "                                for term in found_terms:\n",
    "                                    indices = [i for i, token in enumerate(tokens) if token == term]\n",
    "                                    for index in indices:\n",
    "                                        start = max(0, index - window)\n",
    "                                        end = min(len(tokens), index + window + 1)\n",
    "                                        context = ' '.join(tokens[start:end])\n",
    "                                        # Collect all metadata from the row and the context info\n",
    "                                        result = {\n",
    "                                            \"Text\": row.get('Text', ''),\n",
    "                                            \"Title\": row.get('Title', ''),\n",
    "                                            \"URL\": row.get('URL', ''),\n",
    "                                            \"Date\": row.get('Date', ''),\n",
    "                                            \"Category\": row.get('Category', ''),\n",
    "                                            \"Newspaper\": os.path.basename(dirpath),\n",
    "                                            \"File_Name\": file_name,\n",
    "                                            \"Term\": term,\n",
    "                                            \"Context\": context\n",
    "                                        }\n",
    "                                        writer.writerow(result)\n",
    "    print(f\"Results written to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "country_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Iraq\"  # Directory containing CSV files\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"     # Output directory\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']    # Arabic terms to search for\n",
    "tokenize_search_and_context_to_csv(country_path, output_dir, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make json*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed JSON created at: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\detailed_country_newspapers.json\n"
     ]
    }
   ],
   "source": [
    "def generate_detailed_country_newspaper_json(base_path):\n",
    "    country_newspapers = {}\n",
    "\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        parts = root.split(os.sep)\n",
    "        if len(parts) > 1 and parts[-1].startswith(\"AraNPCC_\"):\n",
    "            country = parts[-1].replace(\"AraNPCC_\", \"\")\n",
    "            newspapers = {}\n",
    "            \n",
    "            for file_name in files:\n",
    "                if file_name.endswith('.csv'):\n",
    "                    # Extract the newspaper name by removing the year and file extension\n",
    "                    newspaper_name = '_'.join(file_name.split('_')[1:-1])\n",
    "                    if newspaper_name not in newspapers:\n",
    "                        newspapers[newspaper_name] = []\n",
    "                    newspapers[newspaper_name].append(file_name)\n",
    "\n",
    "            country_newspapers[country] = newspapers\n",
    "    \n",
    "    json_path = os.path.join(base_path, 'detailed_country_newspapers.json')\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(country_newspapers, json_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    return json_path\n",
    "\n",
    "# Example usage\n",
    "base_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"  # Adjust this path to your directory structure\n",
    "json_path = generate_detailed_country_newspaper_json(base_path)\n",
    "print(\"Detailed JSON created at:\", json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'كوفيد', 'وباء', 'فيروس'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV field size limit set to 2147483647\n",
      "Creating file for adenalghad: Yemen_adenalghad_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_adenalghad_search_results.csv\n",
      "Creating file for aleshteraki: Yemen_aleshteraki_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_aleshteraki_search_results.csv\n",
      "Creating file for almashhad: Yemen_almashhad_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_almashhad_search_results.csv\n",
      "Creating file for almotamar: Yemen_almotamar_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_almotamar_search_results.csv\n",
      "Creating file for alsahwa: Yemen_alsahwa_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_alsahwa_search_results.csv\n",
      "Creating file for alwahdawi: Yemen_alwahdawi_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_alwahdawi_search_results.csv\n",
      "Creating file for marebpress: Yemen_marebpress_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_marebpress_search_results.csv\n",
      "Creating file for saadahpress: Yemen_saadahpress_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_saadahpress_search_results.csv\n",
      "Creating file for samaa: Yemen_samaa_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_samaa_search_results.csv\n",
      "Creating file for yemensaeed: Yemen_yemensaeed_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Yemen_yemensaeed_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def set_max_csv_field_size():\n",
    "    max_int_c_long = 2147483647\n",
    "    try:\n",
    "        csv.field_size_limit(max_int_c_long)\n",
    "        print(f\"CSV field size limit set to {max_int_c_long}\")\n",
    "    except OverflowError as e:\n",
    "        print(\"OverflowError encountered while setting field size limit:\", e)\n",
    "\n",
    "def load_json_reference(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def tokenize_search_and_context_to_csv(json_reference_path, output_dir, country_name, search_terms, window=10):\n",
    "    set_max_csv_field_size()\n",
    "    search_terms_set = set(search_terms)\n",
    "    results_header = [\"Text\", \"Title\", \"URL\", \"Date\", \"Category\", \"Newspaper\", \"File_Name\", \"Term\", \"Context\"]\n",
    "\n",
    "    # Load JSON reference\n",
    "    country_newspapers = load_json_reference(json_reference_path)\n",
    "    newspapers = country_newspapers.get(country_name, {})\n",
    "\n",
    "    for newspaper, files in newspapers.items():\n",
    "        output_file_name = f\"{country_name}_{newspaper}_search_results.csv\"\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "        with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=results_header)\n",
    "            writer.writeheader()\n",
    "            print(f\"Creating file for {newspaper}: {output_file_name}\")\n",
    "\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(output_dir, f\"AraNPCC_{country_name}\", filename)\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"File not found: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "                    csv_reader = csv.DictReader(infile, delimiter='\\t')\n",
    "                    for row in csv_reader:\n",
    "                        text = row.get('Text', '').lower()\n",
    "                        tokens = word_tokenize(text)\n",
    "                        found_terms = search_terms_set.intersection(tokens)\n",
    "                        if found_terms:\n",
    "                            for term in found_terms:\n",
    "                                indices = [i for i, token in enumerate(tokens) if token == term]\n",
    "                                for index in indices:\n",
    "                                    start = max(0, index - window)\n",
    "                                    end = min(len(tokens), index + window + 1)\n",
    "                                    context = ' '.join(tokens[start:end])\n",
    "                                    result = {\n",
    "                                        \"Text\": row.get('Text', ''),\n",
    "                                        \"Title\": row.get('Title', ''),\n",
    "                                        \"URL\": row.get('URL', ''),\n",
    "                                        \"Date\": row.get('Date', ''),\n",
    "                                        \"Category\": row.get('Category', ''),\n",
    "                                        \"Newspaper\": row.get('Newspaper', ''),\n",
    "                                        \"File_Name\": filename,\n",
    "                                        \"Term\": term,\n",
    "                                        \"Context\": context\n",
    "                                    }\n",
    "                                    writer.writerow(result)\n",
    "            print(f\"Results written to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "json_reference_path = r'C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\detailed_country_newspapers.json'\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"\n",
    "country_name = 'Yemen'\n",
    "search_terms = ['كوفيد', 'وباء', 'فيروس']\n",
    "tokenize_search_and_context_to_csv(json_reference_path, output_dir, country_name, search_terms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter articles relevant to covid 'كوفيد', \"كورونا\"*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-12 13:36:11.515539: CSV field size limit set to 2147483647\n",
      "2024-05-12 13:36:11.517069: Creating file for ahramgate: Egypt_ahramgate_search_results.csv\n",
      "2024-05-12 13:36:11.517069: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_ahramgate_19.csv\n",
      "2024-05-12 13:37:17.111615: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_ahramgate_20.csv\n",
      "2024-05-12 13:38:58.213804: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_ahramgate_21.csv\n",
      "2024-05-12 13:40:04.143502: Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Egypt_ahramgate_search_results.csv\n",
      "2024-05-12 13:40:04.152048: Creating file for akhbarelyomgate: Egypt_akhbarelyomgate_search_results.csv\n",
      "2024-05-12 13:40:04.152048: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_akhbarelyomgate_19.csv\n",
      "2024-05-12 13:42:19.993054: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_akhbarelyomgate_20.csv\n",
      "2024-05-12 13:45:03.140060: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_akhbarelyomgate_21.csv\n",
      "2024-05-12 13:51:15.900029: Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Egypt_akhbarelyomgate_search_results.csv\n",
      "2024-05-12 13:51:15.929022: Creating file for alwafd: Egypt_alwafd_search_results.csv\n",
      "2024-05-12 13:51:15.929022: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_alwafd_19.csv\n",
      "2024-05-12 13:54:10.157529: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_alwafd_20.csv\n",
      "2024-05-12 13:58:09.108006: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_alwafd_21.csv\n",
      "2024-05-12 14:02:14.438765: Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Egypt_alwafd_search_results.csv\n",
      "2024-05-12 14:02:14.467252: Creating file for elbalad: Egypt_elbalad_search_results.csv\n",
      "2024-05-12 14:02:14.467252: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_elbalad_19.csv\n",
      "2024-05-12 14:03:38.022249: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_elbalad_20.csv\n",
      "2024-05-12 14:05:05.409698: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_elbalad_21.csv\n",
      "2024-05-12 14:06:29.653014: Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Egypt_elbalad_search_results.csv\n",
      "2024-05-12 14:06:29.660988: Creating file for shorouk: Egypt_shorouk_search_results.csv\n",
      "2024-05-12 14:06:29.660988: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_shorouk_19.csv\n",
      "2024-05-12 14:09:01.262217: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_shorouk_20.csv\n",
      "2024-05-12 14:10:23.095082: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_shorouk_21.csv\n",
      "2024-05-12 14:12:29.861091: Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Egypt_shorouk_search_results.csv\n",
      "2024-05-12 14:12:29.868441: Creating file for youm7: Egypt_youm7_search_results.csv\n",
      "2024-05-12 14:12:29.868441: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_youm7_19.csv\n",
      "2024-05-12 14:15:51.287627: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_youm7_20.csv\n",
      "2024-05-12 14:23:12.264339: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\AraNPCC_Egypt\\Egypt_youm7_21.csv\n",
      "2024-05-12 14:27:12.009551: Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\Egypt_youm7_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "def set_max_csv_field_size():\n",
    "    max_int_c_long = 2147483647\n",
    "    try:\n",
    "        csv.field_size_limit(max_int_c_long)\n",
    "        print(f\"{datetime.now()}: CSV field size limit set to {max_int_c_long}\")\n",
    "    except OverflowError as e:\n",
    "        print(f\"{datetime.now()}: OverflowError encountered while setting field size limit:\", e)\n",
    "\n",
    "def load_json_reference(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "def tokenize_search_and_context_to_csv(json_reference_path, output_dir, country_name, search_terms, window=10):\n",
    "    set_max_csv_field_size()\n",
    "    search_terms_set = set(search_terms)\n",
    "    results_header = [\"Text\", \"Title\", \"URL\", \"Date\", \"Category\", \"Newspaper\", \"File_Name\", \"Term\"]\n",
    "\n",
    "    # Load JSON reference\n",
    "    country_newspapers = load_json_reference(json_reference_path)\n",
    "    newspapers = country_newspapers.get(country_name, {})\n",
    "\n",
    "    for newspaper, files in newspapers.items():\n",
    "        output_file_name = f\"{country_name}_{newspaper}_search_results.csv\"\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "        processed_articles = set()\n",
    "\n",
    "        with open(output_file_path, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=results_header)\n",
    "            writer.writeheader()\n",
    "            print(f\"{datetime.now()}: Creating file for {newspaper}: {output_file_name}\")\n",
    "\n",
    "            for filename in files:\n",
    "                file_path = os.path.join(output_dir, f\"AraNPCC_{country_name}\", filename)\n",
    "                print(f\"{datetime.now()}: Processing file: {file_path}\")\n",
    "                if not os.path.exists(file_path):\n",
    "                    print(f\"{datetime.now()}: File not found: {file_path}\")\n",
    "                    continue\n",
    "\n",
    "                with open(file_path, mode='r', encoding='utf-8') as infile:\n",
    "                    csv_reader = csv.DictReader(infile, delimiter='\\t')\n",
    "                    for row in csv_reader:\n",
    "                        text = row.get('Text', '').lower()\n",
    "                        tokens = word_tokenize(text)\n",
    "                        found_terms = search_terms_set.intersection(tokens)\n",
    "                        \n",
    "                        article_key = (row.get('Title', ''), row.get('Date', ''), newspaper)\n",
    "                        if found_terms and article_key not in processed_articles:\n",
    "                            processed_articles.add(article_key)\n",
    "                            result = {\n",
    "                                \"Text\": text,\n",
    "                                \"Title\": row.get('Title', ''),\n",
    "                                \"URL\": row.get('URL', ''),\n",
    "                                \"Date\": row.get('Date', ''),\n",
    "                                \"Category\": row.get('Category', ''),\n",
    "                                \"Newspaper\": newspaper,\n",
    "                                \"File_Name\": filename,\n",
    "                                \"Term\": \", \".join(found_terms)\n",
    "                            }\n",
    "                            writer.writerow(result)\n",
    "            print(f\"{datetime.now()}: Results written to {output_file_path}\")\n",
    "\n",
    "# Example usage\n",
    "json_reference_path = r'C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\detailed_country_newspapers.json'\n",
    "output_dir = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\"\n",
    "country_name = 'Egypt'\n",
    "search_terms = ['كوفيد', \"كورونا\"]\n",
    "tokenize_search_and_context_to_csv(json_reference_path, output_dir, country_name, search_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clean csv files, date column*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-12 14:54:13.854282: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_ahramgate_search_results.csv\n",
      "2024-05-12 14:54:17.816873: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_ahramgate_search_results.csv\n",
      "2024-05-12 14:54:17.816873: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_akhbarelyomgate_search_results.csv\n",
      "2024-05-12 14:54:31.745828: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_akhbarelyomgate_search_results.csv\n",
      "2024-05-12 14:54:31.746348: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_alwafd_search_results.csv\n",
      "2024-05-12 14:54:43.997167: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_alwafd_search_results.csv\n",
      "2024-05-12 14:54:43.997663: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_elbalad_search_results.csv\n",
      "2024-05-12 14:54:51.416900: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_elbalad_search_results.csv\n",
      "2024-05-12 14:54:51.417895: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_shorouk_search_results.csv\n",
      "2024-05-12 14:54:57.152519: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_shorouk_search_results.csv\n",
      "2024-05-12 14:54:57.153016: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_youm7_search_results.csv\n",
      "2024-05-12 14:55:37.103798: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_youm7_search_results.csv\n",
      "2024-05-12 14:55:37.103798: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_ahdathpress_search_results.csv\n",
      "2024-05-12 14:55:38.289852: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_ahdathpress_search_results.csv\n",
      "2024-05-12 14:55:38.290348: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_al9anat_search_results.csv\n",
      "2024-05-12 14:55:38.692888: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_al9anat_search_results.csv\n",
      "2024-05-12 14:55:38.693384: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alalam_search_results.csv\n",
      "2024-05-12 14:55:39.001888: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alalam_search_results.csv\n",
      "2024-05-12 14:55:39.002880: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alittihad_search_results.csv\n",
      "2024-05-12 14:55:39.920028: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alittihad_search_results.csv\n",
      "2024-05-12 14:55:39.920524: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_almaghribia_search_results.csv\n",
      "2024-05-12 14:55:40.734414: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_almaghribia_search_results.csv\n",
      "2024-05-12 14:55:40.734910: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alyaoum24_search_results.csv\n",
      "2024-05-12 14:55:42.177378: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alyaoum24_search_results.csv\n",
      "2024-05-12 14:55:42.177378: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_bayanealyaoume_search_results.csv\n",
      "2024-05-12 14:55:43.312322: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_bayanealyaoume_search_results.csv\n",
      "2024-05-12 14:55:43.312818: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_hespress_search_results.csv\n",
      "2024-05-12 14:55:46.324341: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_hespress_search_results.csv\n",
      "2024-05-12 14:55:46.324838: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_adenalghad_search_results.csv\n",
      "2024-05-12 14:55:47.950246: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_adenalghad_search_results.csv\n",
      "2024-05-12 14:55:47.950742: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_aleshteraki_search_results.csv\n",
      "2024-05-12 14:55:48.049366: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_aleshteraki_search_results.csv\n",
      "2024-05-12 14:55:48.049366: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almashhad_search_results.csv\n",
      "2024-05-12 14:55:49.124880: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almashhad_search_results.csv\n",
      "2024-05-12 14:55:49.124880: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almotamar_search_results.csv\n",
      "2024-05-12 14:55:49.494278: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almotamar_search_results.csv\n",
      "2024-05-12 14:55:49.494278: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alsahwa_search_results.csv\n",
      "2024-05-12 14:55:49.697370: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alsahwa_search_results.csv\n",
      "2024-05-12 14:55:49.697370: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alwahdawi_search_results.csv\n",
      "2024-05-12 14:55:49.784893: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alwahdawi_search_results.csv\n",
      "2024-05-12 14:55:49.784893: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_marebpress_search_results.csv\n",
      "2024-05-12 14:55:50.220333: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_marebpress_search_results.csv\n",
      "2024-05-12 14:55:50.220830: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_saadahpress_search_results.csv\n",
      "2024-05-12 14:55:50.289373: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_saadahpress_search_results.csv\n",
      "2024-05-12 14:55:50.289868: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_samaa_search_results.csv\n",
      "2024-05-12 14:55:50.371580: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_samaa_search_results.csv\n",
      "2024-05-12 14:55:50.371580: Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_yemensaeed_search_results.csv\n",
      "2024-05-12 14:55:54.817410: Cleaned and saved: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_yemensaeed_search_results.csv\n"
     ]
    }
   ],
   "source": [
    "def clean_date(date_str):\n",
    "    # Strip out unwanted characters [' and '] from the date string\n",
    "    return date_str.strip(\"[]'\")\n",
    "\n",
    "def process_files(directory):\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            print(f\"{datetime.now()}: Processing file: {file_path}\")\n",
    "            \n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Check if 'Date' column exists in the DataFrame\n",
    "            if 'Date' in df.columns:\n",
    "                # Apply the cleaning function to the 'Date' column\n",
    "                df['Date'] = df['Date'].apply(clean_date)\n",
    "                \n",
    "                # Save the cleaned DataFrame back to CSV\n",
    "                df.to_csv(file_path, index=False)\n",
    "                print(f\"{datetime.now()}: Cleaned and saved: {file_path}\")\n",
    "            else:\n",
    "                print(f\"{datetime.now()}: No 'Date' column found in: {file_path}\")\n",
    "\n",
    "# Specify the directory containing your CSV files\n",
    "directory = r'C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles'\n",
    "process_files(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count articles of filtered dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_ahramgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_akhbarelyomgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_alwafd_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_elbalad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_shorouk_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_youm7_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_ahdathpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_al9anat_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alalam_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alittihad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_almaghribia_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alyaoum24_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_bayanealyaoume_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_hespress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_adenalghad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_aleshteraki_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almashhad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almotamar_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alsahwa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alwahdawi_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_marebpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_saadahpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_samaa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_yemensaeed_search_results.csv\n",
      "Article counts saved to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\covid_article_counts.csv\n"
     ]
    }
   ],
   "source": [
    "def count_articles(directory):\n",
    "    \"\"\" Count articles per country using directory structure and file names. \"\"\"\n",
    "    article_counts = {}\n",
    "    # Loop over every file in the specified directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Assuming file names are in the format \"Country_newspaper_search_results.csv\"\n",
    "            country = file_name.split('_')[0]  # Extract the country name from the file name\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            if os.path.exists(file_path):\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    article_count = len(df)\n",
    "                    if country in article_counts:\n",
    "                        article_counts[country] += article_count\n",
    "                    else:\n",
    "                        article_counts[country] = article_count\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "            else:\n",
    "                print(f\"File not found: {file_path}\")\n",
    "\n",
    "    return article_counts\n",
    "\n",
    "def save_counts_to_csv(article_counts, output_file):\n",
    "    \"\"\" Save the article counts to a CSV file. \"\"\"\n",
    "    df = pd.DataFrame(list(article_counts.items()), columns=['Country', 'ArticleCount'])\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Article counts saved to {output_file}\")\n",
    "\n",
    "# Directory containing the COVID-related article CSV files\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\"\n",
    "\n",
    "# Path to save the output CSV file with article counts\n",
    "output_csv_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\covid_article_counts.csv\"\n",
    "\n",
    "# Count articles per country\n",
    "article_counts = count_articles(directory)\n",
    "\n",
    "# Save the results to CSV\n",
    "save_counts_to_csv(article_counts, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count articles of filtered dataset over time and by newspaper*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_ahramgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_akhbarelyomgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_alwafd_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_elbalad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_shorouk_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_youm7_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_ahdathpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_al9anat_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alalam_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alittihad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_almaghribia_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alyaoum24_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_bayanealyaoume_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_hespress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_adenalghad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_aleshteraki_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almashhad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almotamar_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alsahwa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alwahdawi_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_marebpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_saadahpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_samaa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_yemensaeed_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\covid_article_counts_by_date.csv\n"
     ]
    }
   ],
   "source": [
    "def parse_date(date_str):\n",
    "    \"\"\"Attempt to parse the date with different expected formats.\"\"\"\n",
    "    for fmt in ('%d-%m-%Y', '%m-%d-%Y', '%Y-%m-%d'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return pd.NaT  # Return Not a Time (NaT) if all formats fail\n",
    "\n",
    "def process_files(directory):\n",
    "    results = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            country, newspaper = parse_filename(file_name)\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                # Apply robust date parsing\n",
    "                df['Date'] = df['Date'].apply(parse_date)\n",
    "                df_grouped = df.groupby(df['Date'].dt.to_period('D')).size().reset_index(name='ArticleCount')\n",
    "                df_grouped['Country'] = country\n",
    "                df_grouped['Newspaper'] = newspaper\n",
    "                results.append(df_grouped)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    return final_df\n",
    "\n",
    "def save_results_to_csv(final_df, output_file):\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results written to {output_file}\")\n",
    "\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\"\n",
    "output_csv_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\covid_article_counts_by_date.csv\"\n",
    "final_aggregated_data = process_files(directory)\n",
    "save_results_to_csv(final_aggregated_data, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specific Keyword frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_ahramgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_akhbarelyomgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_alwafd_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_elbalad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_shorouk_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_youm7_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_ahdathpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_al9anat_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alalam_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alittihad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_almaghribia_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alyaoum24_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_bayanealyaoume_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_hespress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_adenalghad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_aleshteraki_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almashhad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almotamar_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alsahwa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alwahdawi_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_marebpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_saadahpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_samaa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_yemensaeed_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\covid_article_counts_by_date.csv\n"
     ]
    }
   ],
   "source": [
    "def parse_filename(file_name):\n",
    "    \"\"\" Extract country and newspaper from filename. \"\"\"\n",
    "    parts = file_name.split('_')\n",
    "    country = parts[0]\n",
    "    newspaper = '_'.join(parts[1:-2])  # Assuming the last part is date or sequence number\n",
    "    return country, newspaper\n",
    "\n",
    "def parse_date(date_str):\n",
    "    \"\"\" Attempt to parse the date with different expected formats. \"\"\"\n",
    "    for fmt in ('%d-%m-%Y', '%m-%d-%Y', '%Y-%m-%d'):\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    return pd.NaT  # Return Not a Time (NaT) if all formats fail\n",
    "\n",
    "def process_files(directory, keywords):\n",
    "    \"\"\" Process each file and count occurrences of keywords. \"\"\"\n",
    "    results = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            country, newspaper = parse_filename(file_name)\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Date'] = df['Date'].apply(parse_date)\n",
    "                \n",
    "                # Initialize counts for each keyword\n",
    "                for keyword in keywords:\n",
    "                    df[keyword] = df['Text'].str.contains(keyword, case=False, na=False)\n",
    "\n",
    "                # Sum up counts by date\n",
    "                df_grouped = df.groupby(df['Date'].dt.to_period('D'))[keywords].sum().reset_index()\n",
    "                df_grouped['Country'] = country\n",
    "                df_grouped['Newspaper'] = newspaper\n",
    "                results.append(df_grouped)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Combine all dataframes\n",
    "    if results:\n",
    "        final_df = pd.concat(results, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return empty dataframe if no results\n",
    "\n",
    "def save_results_to_csv(final_df, output_file):\n",
    "    \"\"\" Save the aggregated results to a CSV file. \"\"\"\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Results written to {output_file}\")\n",
    "\n",
    "# Directory containing CSV files\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\"\n",
    "# Output CSV path\n",
    "output_csv_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\specific_keyword_frequency.csv\"\n",
    "# Keywords to search\n",
    "keywords = ['كوفيد', 'كورونا', 'جائحة', 'وباء', 'لقاح', 'تباعد', 'عزل', 'حظر', 'تعافي', 'وفاة']\n",
    "\n",
    "# Process files and count keyword occurrences\n",
    "final_aggregated_data = process_files(directory, keywords)\n",
    "# Save the results to a CSV file\n",
    "save_results_to_csv(final_aggregated_data, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Keyword frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
      "C:\\Users\\khali\\AppData\\Local\\Temp\\ipykernel_56124\\734671260.py:32: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequencies saved to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\global_keyword_frequencies.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Function to clean Arabic text\n",
    "def clean_arabic_text(text):\n",
    "    text = re.sub(r'[\\u064B-\\u065F]', '', text)  # Remove Arabic diacritics\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    return text\n",
    "\n",
    "# Load stop words\n",
    "def load_stop_words(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = file.read().splitlines()\n",
    "    return set(stop_words)\n",
    "\n",
    "# Function to calculate keyword frequencies\n",
    "def calculate_keyword_frequencies(directory, stop_words):\n",
    "    keyword_frequencies_by_country = {}\n",
    "\n",
    "    # Iterate through all files in the specified directory\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            country_name = file_name.split('_')[0]  # Extract country name from the file name\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Month'] = pd.to_datetime(df['Date'], errors='coerce').dt.to_period('M')\n",
    "                if country_name not in keyword_frequencies_by_country:\n",
    "                    keyword_frequencies_by_country[country_name] = {}\n",
    "                for month, group in df.groupby('Month'):\n",
    "                    texts = group['Text'].dropna()  # Drop missing values\n",
    "                    monthly_frequencies = Counter()\n",
    "                    for text in texts:\n",
    "                        text = clean_arabic_text(text)\n",
    "                        tokens = word_tokenize(text)\n",
    "                        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "                        monthly_frequencies.update(filtered_tokens)\n",
    "                    keyword_frequencies_by_country[country_name][str(month)] = Counter({word: count for word, count in monthly_frequencies.items() if count >= 10})\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {file_path}: {e}\")\n",
    "    \n",
    "    return keyword_frequencies_by_country\n",
    "\n",
    "# Function to save the frequencies to an Excel file\n",
    "def save_frequencies_to_excel(keyword_frequencies_by_country, output_file):\n",
    "    workbook = Workbook()\n",
    "    for country, monthly_frequencies in keyword_frequencies_by_country.items():\n",
    "        sheet = workbook.create_sheet(title=country)\n",
    "        sheet.append(['Month', 'Keyword', 'Frequency'])\n",
    "        for month, frequencies in monthly_frequencies.items():\n",
    "            for keyword, frequency in frequencies.items():\n",
    "                sheet.append([month, keyword, frequency])\n",
    "    \n",
    "    # Remove the default sheet created by Workbook\n",
    "    default_sheet = workbook['Sheet']\n",
    "    workbook.remove(default_sheet)\n",
    "\n",
    "    workbook.save(output_file)\n",
    "    print(f\"Frequencies saved to {output_file}\")\n",
    "\n",
    "# Usage\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\"  # Directory containing the CSV files\n",
    "stop_words_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\stop_words.txt\"  # Path to the stop words file\n",
    "output_excel_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\global_keyword_frequencies.xlsx\"\n",
    "\n",
    "stop_words = load_stop_words(stop_words_path)\n",
    "keyword_frequencies_by_country = calculate_keyword_frequencies(directory, stop_words)\n",
    "save_frequencies_to_excel(keyword_frequencies_by_country, output_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "attmept at collocates over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Morocco_alalam_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_aleshteraki_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_alsahwa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_alwahdawi_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_saadahpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_samaa_search_results.csv\n",
      "Results written to C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocation_test.csv\n"
     ]
    }
   ],
   "source": [
    "def parse_filename(file_name):\n",
    "    \"\"\" Extract country and newspaper from filename. Assumes format Country_Newspaper_Date.csv \"\"\"\n",
    "    parts = file_name.split('_')\n",
    "    country = parts[0]\n",
    "    newspaper = parts[1]\n",
    "    return country, newspaper\n",
    "\n",
    "def find_collocations(text, keyword, window_size):\n",
    "    \"\"\" Find collocations around a specified keyword within the given window size. \"\"\"\n",
    "    text = strip_tashkeel(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    collocations = Counter()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == keyword:\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(tokens), i + window_size + 1)\n",
    "            window_tokens = tokens[start:i] + tokens[i+1:end]\n",
    "            for gram in window_tokens:\n",
    "                collocations[gram] += 1\n",
    "    return collocations\n",
    "\n",
    "def process_files(directory, keyword, window_size):\n",
    "    \"\"\" Process each file to find collocations for the specified keyword, aggregated by month. \"\"\"\n",
    "    results = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            country, newspaper = parse_filename(file_name)\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Country'] = country  # Add country information to DataFrame\n",
    "                df['Newspaper'] = newspaper  # Add newspaper information to DataFrame\n",
    "                df['Month'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True).dt.to_period('M')\n",
    "                grouped = df.groupby(['Month', 'Country', 'Newspaper'])\n",
    "                for (month, country, newspaper), group in grouped:\n",
    "                    all_text = ' '.join(group['Text'].dropna())\n",
    "                    collocations = find_collocations(all_text, keyword, window_size)\n",
    "                    # Flatten the collocations into columns\n",
    "                    col_dict = {f\"Top {i+1}\": f\"{word} ({count})\" for i, (word, count) in enumerate(collocations.most_common(10))}\n",
    "                    results.append({\n",
    "                        'Month': str(month),\n",
    "                        'Country': country,\n",
    "                        'Newspaper': newspaper,\n",
    "                        **col_dict\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def save_results_to_csv(results_df, output_file):\n",
    "    \"\"\" Save the results to a CSV file. \"\"\"\n",
    "    if results_df.empty:\n",
    "        print(\"No data to save. Check the processing steps.\")\n",
    "        return\n",
    "    \n",
    "    results_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Results written to {output_file}\")\n",
    "\n",
    "# Directory and parameters setup\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\"\n",
    "keyword = 'فيروس'  # Focus on 'virus' as the keyword\n",
    "window_size = 1  # Number of words before and after the keyword\n",
    "output_csv_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocation_test.csv\"\n",
    "\n",
    "# Process files and save results\n",
    "final_results = process_files(directory, keyword, window_size)\n",
    "save_results_to_csv(final_results, output_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation frequency by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Morocco_alalam_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_aleshteraki_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_alsahwa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_alwahdawi_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_saadahpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\\Yemen_samaa_search_results.csv\n",
      "Results for Morocco written to sheet in C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocation_test_2.xlsx\n",
      "Results for Yemen written to sheet in C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocation_test_2.xlsx\n"
     ]
    }
   ],
   "source": [
    "def parse_filename(file_name):\n",
    "    \"\"\" Extract country from filename. Assumes format Country_Newspaper_Date.csv \"\"\"\n",
    "    return file_name.split('_')[0]\n",
    "\n",
    "def find_collocations(text, keyword, window_size):\n",
    "    \"\"\" Find collocations around a specified keyword within the given window size. \"\"\"\n",
    "    text = strip_tashkeel(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    collocations = Counter()\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == keyword:\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(tokens), i + window_size + 1)\n",
    "            window_tokens = tokens[start:i] + tokens[i+1:end]\n",
    "            for gram in window_tokens:\n",
    "                collocations[gram] += 1\n",
    "    return collocations\n",
    "\n",
    "def process_files(directory, keyword, window_size):\n",
    "    \"\"\" Process each file to find collocations for the specified keyword, summed by country. \"\"\"\n",
    "    country_collocations = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            country = parse_filename(file_name)\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                all_text = ' '.join(df['Text'].dropna())\n",
    "                collocations = find_collocations(all_text, keyword, window_size)\n",
    "                if country not in country_collocations:\n",
    "                    country_collocations[country] = Counter()\n",
    "                country_collocations[country].update(collocations)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return country_collocations\n",
    "\n",
    "def save_results_to_excel(country_collocations, output_file):\n",
    "    \"\"\" Save the results to an Excel file with each country's collocations on separate sheets. \"\"\"\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        for country, collocations in country_collocations.items():\n",
    "            df = pd.DataFrame(collocations.items(), columns=['Collocation', 'Frequency'])\n",
    "            df.sort_values(by='Frequency', ascending=False, inplace=True)\n",
    "            df.to_excel(writer, sheet_name=country, index=False)\n",
    "            print(f\"Results for {country} written to sheet in {output_file}\")\n",
    "\n",
    "# Directory and parameters setup\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\test\"\n",
    "keyword = 'فيروس'  # Focus on 'virus' as the keyword\n",
    "window_size = 2  # Number of words before and after the keyword\n",
    "output_excel_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocation_test_2.xlsx\"\n",
    "\n",
    "# Process files and save results\n",
    "country_collocations = process_files(directory, keyword, window_size)\n",
    "save_results_to_excel(country_collocations, output_excel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collocation frequency by country by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_ahramgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_akhbarelyomgate_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_alwafd_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_elbalad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_shorouk_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Egypt_youm7_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_ahdathpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_al9anat_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alalam_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alittihad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_almaghribia_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_alyaoum24_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_bayanealyaoume_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Morocco_hespress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_adenalghad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_aleshteraki_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almashhad_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_almotamar_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alsahwa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_alwahdawi_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_marebpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_saadahpress_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_samaa_search_results.csv\n",
      "Processing file: C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\\Yemen_yemensaeed_search_results.csv\n",
      "Results for Egypt written to sheet in C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocations.xlsx\n",
      "Results for Morocco written to sheet in C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocations.xlsx\n",
      "Results for Yemen written to sheet in C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocations.xlsx\n"
     ]
    }
   ],
   "source": [
    "def parse_filename(file_name):\n",
    "    \"\"\" Extract country from filename. Assumes format Country_Newspaper_Date.csv \"\"\"\n",
    "    return file_name.split('_')[0]\n",
    "\n",
    "def clean_token(token, punctuation):\n",
    "    \"\"\" Clean token by removing leading and trailing punctuation. \"\"\"\n",
    "    return re.sub(r'^[' + punctuation + ']+|[' + punctuation + ']+$', '', token)\n",
    "\n",
    "def load_stop_words(file_path):\n",
    "    \"\"\" Load stop words from a file. \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        stop_words = set(file.read().splitlines())\n",
    "    return stop_words\n",
    "\n",
    "def find_collocations(text, keyword, window_size, stop_words):\n",
    "    \"\"\" Find collocations around a specified keyword within the given window size, ignoring punctuation and stop words. \"\"\"\n",
    "    text = strip_tashkeel(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    collocations = Counter()\n",
    "    punctuation = string.punctuation + \"“”‘’—،–\"\n",
    "\n",
    "    for i, token in enumerate(tokens):\n",
    "        cleaned_token = clean_token(token, punctuation)\n",
    "        if cleaned_token == keyword:\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(tokens), i + window_size + 1)\n",
    "            # Exclude tokens that are entirely punctuation or stop words\n",
    "            window_tokens = [clean_token(t, punctuation) for t in tokens[start:i] + tokens[i+1:end] if not all(char in punctuation for char in t) and t not in stop_words]\n",
    "            for gram in window_tokens:\n",
    "                if gram:  # Ensure it's not empty after cleaning\n",
    "                    collocations[gram] += 1\n",
    "    return collocations\n",
    "\n",
    "def process_files(directory, keyword, window_size, stop_words):\n",
    "    \"\"\" Process each file to find collocations for the specified keyword, summed by country and month, ignoring punctuation. \"\"\"\n",
    "    country_collocations = defaultdict(lambda: defaultdict(Counter))\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            country = parse_filename(file_name)\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                df['Month'] = pd.to_datetime(df['Date'], errors='coerce', dayfirst=True).dt.to_period('M')\n",
    "                for (month), group in df.groupby('Month'):\n",
    "                    all_text = ' '.join(group['Text'].dropna())\n",
    "                    collocations = find_collocations(all_text, keyword, window_size, stop_words)\n",
    "                    country_collocations[country][month].update(collocations)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "    return country_collocations\n",
    "\n",
    "def save_results_to_excel(country_collocations, output_file):\n",
    "    \"\"\" Save the results to an Excel file with each country's collocations on separate sheets, organized by month. \"\"\"\n",
    "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "        for country, months_data in country_collocations.items():\n",
    "            rows = []\n",
    "            for month, collocations in months_data.items():\n",
    "                for col, freq in collocations.most_common(10):\n",
    "                    rows.append({\n",
    "                        'Month': str(month),\n",
    "                        'Collocation': col,\n",
    "                        'Frequency': freq\n",
    "                    })\n",
    "            if rows:\n",
    "                df = pd.DataFrame(rows)\n",
    "                df.sort_values(by=['Month', 'Frequency'], ascending=[True, False], inplace=True)\n",
    "                df.to_excel(writer, sheet_name=country, index=False)\n",
    "                print(f\"Results for {country} written to sheet in {output_file}\")\n",
    "            else:\n",
    "                print(f\"No data for {country}.\")\n",
    "\n",
    "# Directory and parameters setup\n",
    "directory = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\COVID_Articles\"\n",
    "keyword = 'فيروس'  # Focus on 'virus' as the keyword\n",
    "window_size = 5  # Number of words before and after the keyword\n",
    "output_excel_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\collocations.xlsx\"\n",
    "stop_words_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\stop_words.txt\"  # Path to the stop words file\n",
    "\n",
    "# Load stop words\n",
    "arabic_stop_words = load_stop_words(stop_words_path)\n",
    "\n",
    "# Process files and save results\n",
    "country_collocations = process_files(directory, keyword, window_size, arabic_stop_words)\n",
    "save_results_to_excel(country_collocations, output_excel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert date strings to proper date format\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        # Convert 'YYYY-MM' to a date format\n",
    "        return pd.to_datetime(date_str, format='%Y-%m')\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "# Function to process the Excel file and update the date column\n",
    "def process_excel_file(file_path, sheet_name, date_column):\n",
    "    # Load the Excel file\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    \n",
    "    # Apply the date conversion function to the specified column\n",
    "    df[date_column] = df[date_column].apply(convert_date_format)\n",
    "    \n",
    "    # Save the updated DataFrame back to the same Excel file using xlsxwriter\n",
    "    with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    print(f\"Dates converted and saved to {file_path}\")\n",
    "\n",
    "# Specify the path to the Excel file, sheet name, and the column to process\n",
    "file_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\global_keyword_frequencies.xlsx\"  # Change to your file path\n",
    "sheet_name = \"Morocco\"  # Change to your sheet name\n",
    "date_column = \"Month\"   # Change to your date column name\n",
    "\n",
    "# Process the Excel file\n",
    "process_excel_file(file_path, sheet_name, date_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert date strings to proper date format\n",
    "def convert_date_format(date_str):\n",
    "    try:\n",
    "        # Convert 'YYYY-MM' to a date format\n",
    "        return pd.to_datetime(date_str, format='%Y-%m')\n",
    "    except ValueError:\n",
    "        return pd.NaT\n",
    "\n",
    "# Function to process the Excel file and update the date column\n",
    "def process_excel_file(file_path, sheet_names, date_column):\n",
    "    # Create a dictionary to store dataframes for each sheet\n",
    "    sheets_data = {}\n",
    "\n",
    "    # Load the Excel file and process each sheet\n",
    "    for sheet_name in sheet_names:\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
    "            # Apply the date conversion function to the specified column\n",
    "            df[date_column] = df[date_column].apply(convert_date_format)\n",
    "            sheets_data[sheet_name] = df\n",
    "            print(f\"Processed sheet: {sheet_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process sheet {sheet_name}: {e}\")\n",
    "\n",
    "    # Write the updated DataFrames to the same Excel file\n",
    "    with pd.ExcelWriter(file_path, engine='xlsxwriter') as writer:\n",
    "        for sheet_name, df in sheets_data.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            print(f\"Saved sheet: {sheet_name}\")\n",
    "\n",
    "    print(f\"Dates converted and saved to {file_path}\")\n",
    "\n",
    "# Specify the path to the Excel file, sheet names, and the column to process\n",
    "file_path = r\"C:\\Users\\khali\\OneDrive\\AUS\\Classes\\7 - S24\\ARA 250\\Project\\AraNPCC\\global_keyword_frequencies.xlsx\"  # Change to your file path\n",
    "sheet_names = [\"Egypt\", \"Morocco\", \"Yemen\"]  # List of sheet names to process\n",
    "date_column = \"Month\"   # Change to your date column name\n",
    "\n",
    "# Process the Excel file\n",
    "process_excel_file(file_path, sheet_names, date_column)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c347c8f9a7ef94e4c9e03b4513be7835ed18f45b99a2a817fb579f408b867b16"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
